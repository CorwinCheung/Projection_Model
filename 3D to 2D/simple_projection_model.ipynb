{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('dataset.pkl', 'rb') as f:\n",
    "    loaded_dataset = pickle.load(f)\n",
    "\n",
    "# Verify the shapes of the first 3D matrix and its 2D projection\n",
    "print(loaded_dataset[0][0].shape)  # Shape of the 3D matrix (256, 256, 256)\n",
    "print(loaded_dataset[0][1].shape)  # Shape of the 2D projection (256, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx][0]\n",
    "        y = self.data[idx][1]\n",
    "        return torch.tensor(x, dtype=torch.float32).unsqueeze(0), torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "# Separate the dataset into inputs (3D matrices) and outputs (2D projections)\n",
    "train_data, test_data = train_test_split(loaded_dataset, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = CustomDataset(train_data)\n",
    "test_dataset = CustomDataset(test_data)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN3Dto2D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN3Dto2D, self).__init__()\n",
    "        self.conv1 = nn.Conv3d(1, 32, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool3d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv3d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv3d(64, 128, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(128*32*32*32, 256*256)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = x.view(-1, 128*32*32*32)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = x.view(-1, 256, 256)\n",
    "        return x\n",
    "\n",
    "model = CNN3Dto2D()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f'Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}')\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            val_loss += loss.item()\n",
    "    print(f'Validation Loss: {val_loss/len(test_loader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "test_loss = 0.0\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        test_loss += loss.item()\n",
    "print(f'Test Loss: {test_loss/len(test_loader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helpers: \n",
    "def parse_off(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    assert lines[0].strip() == 'OFF', \"Not a valid OFF file\"\n",
    "\n",
    "    header = lines[1].strip().split()\n",
    "    num_vertices = int(header[0])\n",
    "    num_faces = int(header[1])\n",
    "    \n",
    "    vertices = []\n",
    "    for i in range(2, 2 + num_vertices):\n",
    "        vertex = list(map(float, lines[i].strip().split()))\n",
    "        vertices.append(vertex)\n",
    "    \n",
    "    faces = []\n",
    "    for i in range(2 + num_vertices, 2 + num_vertices + num_faces):\n",
    "        face = list(map(int, lines[i].strip().split()[1:])) # Ignoring the first number (number of vertices in the face)\n",
    "        faces.append(face)\n",
    "    \n",
    "    return vertices, faces\n",
    "\n",
    "def normalize_vertices(vertices, target_dim):\n",
    "    vertices = np.array(vertices)\n",
    "    min_coords = np.min(vertices, axis=0)\n",
    "    max_coords = np.max(vertices, axis=0)\n",
    "    \n",
    "    # Translate vertices to start from (0,0,0)\n",
    "    vertices -= min_coords\n",
    "    \n",
    "    # Scale vertices to fit within the target dimensions\n",
    "    scale = target_dim / np.max(max_coords - min_coords)\n",
    "    vertices *= scale\n",
    "    \n",
    "    return vertices\n",
    "\n",
    "def rasterize_mesh(vertices, faces, matrix_dim):\n",
    "    vertices = normalize_vertices(vertices, matrix_dim)\n",
    "    \n",
    "    # Ensuring the matrix dimensions\n",
    "    matrix = np.zeros((matrix_dim, matrix_dim, matrix_dim), dtype=np.uint8)\n",
    "    \n",
    "    for face in faces:\n",
    "        for vertex_idx in face:\n",
    "            x, y, z = np.round(vertices[vertex_idx]).astype(int)\n",
    "            x = np.clip(x, 0, matrix_dim - 1)\n",
    "            y = np.clip(y, 0, matrix_dim - 1)\n",
    "            z = np.clip(z, 0, matrix_dim - 1)\n",
    "            matrix[x, y, z] = 1\n",
    "    \n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_off_file(file_path, matrix_dim):\n",
    "    vertices, faces = parse_off(file_path)\n",
    "    matrix = rasterize_mesh(vertices, faces, matrix_dim)\n",
    "    return matrix\n",
    "\n",
    "# Load the test file\n",
    "test_file_path = 'Test Set/3D files/toilet_0443.off'  # Replace with the path to your test file\n",
    "matrix_dim = 256\n",
    "test_matrix = process_off_file(test_file_path, matrix_dim)\n",
    "test_matrix_exp = torch.tensor(test_matrix, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predicted_2d_projection = model(test_matrix_exp)\n",
    "print(predicted_2d_projection.shape)  # Should be (1, 256, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
