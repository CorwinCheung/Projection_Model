{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mixed Precision Quantization in Neural Networks\n",
    "\n",
    "# Mixed precision quantization is a technique used to reduce the computational\n",
    "# cost and memory footprint of deep learning models by using different numerical\n",
    "# precisions for different parts of the model. This tutorial will cover the \n",
    "# basic concepts and show how to implement mixed precision quantization using PyTorch.\n",
    "\n",
    "# ## Table of Contents\n",
    "\n",
    "# 1. Introduction to Quantization\n",
    "# 2. Benefits of Mixed Precision\n",
    "# 3. Implementing Mixed Precision Quantization in PyTorch\n",
    "# 4. Example: Mixed Precision in a Simple Neural Network\n",
    "# 5. Conclusion\n",
    "\n",
    "# ## 1. Introduction to Quantization\n",
    "\n",
    "# Quantization refers to the process of mapping a large set of input values to a \n",
    "# smaller set, such as converting 32-bit floating-point numbers to 16-bit floating-point\n",
    "# or 8-bit integers. This can significantly reduce the model size and speed up computations.\n",
    "\n",
    "# In mixed precision quantization, different parts of a neural network can be quantized\n",
    "# to different precisions, depending on their sensitivity to quantization errors.\n",
    "\n",
    "# ## 2. Benefits of Mixed Precision\n",
    "\n",
    "# - **Reduced Memory Usage**: Lower precision requires less memory, which can be beneficial \n",
    "#   for deploying models on resource-constrained devices.\n",
    "# - **Increased Speed**: Lower precision arithmetic operations can be executed faster.\n",
    "# - **Maintained Accuracy**: By carefully selecting which parts of the model to quantize, \n",
    "#   it's possible to maintain the model's accuracy while gaining performance improvements.\n",
    "\n",
    "# ## 3. Implementing Mixed Precision Quantization in PyTorch\n",
    "\n",
    "# PyTorch provides native support for mixed precision training through its `torch.cuda.amp`\n",
    "# module. This allows the use of both 32-bit and 16-bit floating-point numbers in a single\n",
    "# model, taking advantage of GPUs' ability to perform fast half-precision calculations.\n",
    "\n",
    "# First, let's ensure you have the required libraries installed.\n",
    "\n",
    "!pip install torch torchvision\n",
    "\n",
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# ## 4. Example: Mixed Precision in a Simple Neural Network\n",
    "\n",
    "# Let's create a simple convolutional neural network and apply mixed precision training.\n",
    "\n",
    "# Define a simple CNN model\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(64*7*7, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64*7*7)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Function to train and evaluate the model\n",
    "def train_and_evaluate(model, device, train_loader, optimizer, epoch, scaler):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        with autocast():\n",
    "            output = model(data)\n",
    "            loss = nn.functional.cross_entropy(output, target)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)}] Loss: {loss.item():.6f}')\n",
    "\n",
    "# Setup for training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "model = SimpleCNN().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(1, 3):  # Run for 2 epochs as an example\n",
    "    train_and_evaluate(model, device, train_loader, optimizer, epoch, scaler)\n",
    "\n",
    "# ## 5. Conclusion\n",
    "\n",
    "# In this tutorial, we've explored how mixed precision quantization works and implemented\n",
    "# it in a simple CNN using PyTorch's AMP module. Mixed precision allows us to achieve\n",
    "# better performance and efficiency in neural network training and inference without \n",
    "# significantly sacrificing accuracy.\n",
    "\n",
    "# You can extend these concepts to more complex models and experiments to fully leverage \n",
    "# the benefits of mixed precision quantization."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
